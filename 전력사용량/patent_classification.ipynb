{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q --upgrade accelerate einops xformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-30T19:51:05.217467Z","iopub.execute_input":"2023-07-30T19:51:05.218002Z","iopub.status.idle":"2023-07-30T19:51:20.386683Z","shell.execute_reply.started":"2023-07-30T19:51:05.217970Z","shell.execute_reply":"2023-07-30T19:51:20.385320Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"batch_size = 16 # 16\nmax_length = 512 # 512\nepoch = 15\ntest_size = 0.01\nthreshold = 0.02\nlearning_rate = 5e-5\npos_weight = 472\ngamma = 1\nseed = 42\n\nimport torch\nimport torch.nn as nn\n\ndef get_loss_fn():\n    return FocalLoss()\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=gamma, pos_weight=pos_weight):\n        super().__init__()\n        self.gamma = gamma\n        self.pos_weight = pos_weight\n\n    def __call__(self, output, label):\n        alpha = torch.where(label, self.pos_weight, 1)\n        p = torch.sigmoid(output)\n        pt = torch.where(label, p, 1-p)\n        loss = - alpha * (1-pt).pow(self.gamma) * pt.log()\n        return loss.mean()\n\n    def __repr__(self):\n        return f\"FocalLoss(gamma={self.gamma}, pos_weight={self.pos_weight})\"","metadata":{"execution":{"iopub.status.busy":"2023-07-30T19:51:20.388653Z","iopub.execute_input":"2023-07-30T19:51:20.389534Z","iopub.status.idle":"2023-07-30T19:51:20.401663Z","shell.execute_reply.started":"2023-07-30T19:51:20.389476Z","shell.execute_reply":"2023-07-30T19:51:20.400786Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport argparse\nimport logging\nimport os\nimport json\nfrom tqdm.auto import tqdm\nfrom pathlib import Path\nimport torch.nn as nn\nimport math\n\nimport random\nimport torch\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\nfrom datasets import load_metric\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import KFold\n\ndef set_seed():\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\ndef load_data():\n    category_df = pd.read_csv('/kaggle/input/test-input/category.csv', dtype={'SSno': str})\n    train_df = pd.read_csv('/kaggle/input/test-input/train.csv')\n    train_set = Dataset.from_pandas(train_df)\n\n    return train_set, category_df\n\ndef load_model():\n    # hyunwoongko/kobart\n    # jaehyeong/koelectra-base-v3-generalized-sentiment-analysis\n    category_df = pd.read_csv('/kaggle/input/test-input/category.csv', dtype={'SSno': str})\n    idx_to_SS = category_df.SSno.values\n    SS_to_idx = {str(cat):idx for idx, cat in enumerate(idx_to_SS)}\n    idx_to_SS = {value: key for key, value in SS_to_idx.items()}\n    \n    model_path = 'hyunwoongko/kobart'\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        pretrained_model_name_or_path = model_path,\n        num_labels = 564,\n        id2label = idx_to_SS,\n        label2id = SS_to_idx,\n        ignore_mismatched_sizes=True,\n    )\n    \n    def initialize_linear(layer):\n        if isinstance(layer, nn.Linear):\n            nn.init.kaiming_uniform_(layer.weight, a=math.sqrt(5))\n            if layer.bias is not None:\n                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(layer.weight)\n                bound = 1 / math.sqrt(fan_in)\n                nn.init.uniform_(layer.bias, -bound, bound)\n\n    model.classification_head.dropout = nn.Identity()\n    initialize_linear(model.classification_head.out_proj)\n    return tokenizer, model\n\ndef preprocess_data(dataset, category_df):\n    idx_to_SS = category_df.SSno.values\n    SS_to_idx = {str(cat):idx for idx, cat in enumerate(idx_to_SS)}\n\n    def preprocess_fn(example):\n        title = example['invention_title']\n        abstract = example['abstract']\n        claims = example['claims']\n\n        #texts = f\"{title}\"#요약: {abstract} 청구항: {claims}\"\n        #texts = f\"{abstract}\"\n        texts = f\"제목: {title} 요약: {abstract} 자세히: {claims}\"\n        labels = torch.zeros(len(SS_to_idx), dtype=torch.bool)\n\n        for SSno in example['SSnos'].split():\n            labels[SS_to_idx[SSno]] = 1\n\n        return {\n            'texts': texts,\n            'labels': labels,\n        }\n    \n    preprocessed = dataset.map(\n        preprocess_fn,\n        remove_columns=[\n            col\n            for col in dataset.column_names\n            if col not in ['documentId']\n        ],\n    )\n    return preprocessed\n\ndef tokenize_data(dataset, tokenizer):\n    def batch_tokenize(batch):\n        tokenized_batch = tokenizer(\n            batch['texts'],\n            max_length=max_length,\n            padding='max_length',\n            truncation=True,\n        )\n        tokenized_batch['documentId'] = batch['documentId']\n        tokenized_batch['labels'] = batch['labels']\n        return tokenized_batch\n    \n    tokenized = dataset.map(\n        batch_tokenize,\n        batched=True,\n    )\n    \n    return tokenized\n\ndef split_data(dataset):\n    dataset = dataset.train_test_split(\n        test_size = test_size,\n        seed = 42,\n    )\n    return dataset\n    \nclass CustomTrainer(Trainer):\n    def __init__(self, *args, loss_fn, metric, **kargs):\n        super().__init__(*args, **kargs)\n        self.loss_fn = loss_fn\n        self.metric = metric\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        outputs = model(\n            input_ids=inputs[\"input_ids\"],\n            attention_mask=inputs[\"attention_mask\"],\n        )\n        loss = self.loss_fn(outputs.logits, inputs[\"labels\"])\n        return (loss, outputs) if return_outputs else loss\n\n\ndef get_trainer(model, tokenizer, dataset, metric):\n    training_args = TrainingArguments(\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        num_train_epochs=epoch,\n        per_device_train_batch_size=batch_size,\n        optim=\"adamw_torch\",\n        learning_rate=learning_rate,\n        warmup_steps=200,\n        output_dir=\"./results\",\n        save_total_limit=3,\n        report_to=list([]),\n    )\n    \n    optimizer = AdamW(model.parameters(), lr=learning_rate)\n    loss_fn = get_loss_fn()\n    trainer = CustomTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=dataset[\"train\"],\n        eval_dataset=dataset[\"test\"],\n        tokenizer=tokenizer,\n        loss_fn=loss_fn,\n        metric = metric,  # Pass the metric here\n    )\n    return trainer\n\nfrom transformers import AdamW\ndef main():\n    set_seed()\n    #df = pd.read_csv('/kaggle/input/test-input/train.csv')\n    #kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    #for train_index, test_index in kf.split(df):\n    # train_data = df.iloc[train_index]\n    dataset, category_df = load_data()\n    tokenizer, model = load_model()\n\n    dataset = preprocess_data(dataset, category_df)\n    dataset = tokenize_data(dataset, tokenizer)\n    dataset = split_data(dataset)\n\n    metric = load_metric(\"f1\")  # Define the metric\n    trainer = get_trainer(model, tokenizer, dataset, metric)  # Pass the metric\n\n    #if os.path.exists('model_checkpoint.pt') and os.path.exists('optimizer_checkpoint.pt'):\n\n    trainer.train('/kaggle/input/gramenandae/kaggle/working/results/checkpoint-37130')\n\nif __name__ == '__main__':\n    main()","metadata":{"execution":{"iopub.status.busy":"2023-07-30T19:51:20.403123Z","iopub.execute_input":"2023-07-30T19:51:20.403642Z","iopub.status.idle":"2023-07-31T01:22:50.687712Z","shell.execute_reply.started":"2023-07-30T19:51:20.403609Z","shell.execute_reply":"2023-07-31T01:22:50.686382Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/337 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f4ee575e6d84318b55b800d8186d86c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.05M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2137d40efe0c45079ef019a2ef487da5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/109 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00e3a2d733534fe1bee8888c2703dcf3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db8555d029344c71890d4bae14e0e761"}},"metadata":{}},{"name":"stderr","text":"You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/496M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a307667a06e46a1aaf560c053723f0f"}},"metadata":{}},{"name":"stderr","text":"Some weights of BartForSequenceClassification were not initialized from the model checkpoint at hyunwoongko/kobart and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/60000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac4952a2adb0460082818abab0f5f9be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/60 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f862be8898ca4387b91dc44b17579f2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b22533de1be243098896dff8fd5b3abe"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nYou're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='55695' max='55695' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [55695/55695 5:29:03, Epoch 15/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>11</td>\n      <td>0.009600</td>\n      <td>0.159412</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.008600</td>\n      <td>0.193064</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.007100</td>\n      <td>0.206906</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.006300</td>\n      <td>0.221778</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.005400</td>\n      <td>0.241073</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"#!zip -r ./checkpoint_5.zip /kaggle/working/results/checkpoint-37130","metadata":{"execution":{"iopub.status.busy":"2023-07-30T19:37:54.303770Z","iopub.execute_input":"2023-07-30T19:37:54.304165Z","iopub.status.idle":"2023-07-30T19:39:12.716797Z","shell.execute_reply.started":"2023-07-30T19:37:54.304134Z","shell.execute_reply":"2023-07-30T19:39:12.715549Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n  adding: kaggle/working/results/checkpoint-37130/ (stored 0%)\n  adding: kaggle/working/results/checkpoint-37130/pytorch_model.bin (deflated 7%)\n  adding: kaggle/working/results/checkpoint-37130/tokenizer.json (deflated 75%)\n  adding: kaggle/working/results/checkpoint-37130/tokenizer_config.json (deflated 30%)\n  adding: kaggle/working/results/checkpoint-37130/special_tokens_map.json (deflated 49%)\n  adding: kaggle/working/results/checkpoint-37130/training_args.bin (deflated 49%)\n  adding: kaggle/working/results/checkpoint-37130/scheduler.pt (deflated 49%)\n  adding: kaggle/working/results/checkpoint-37130/config.json (deflated 73%)\n  adding: kaggle/working/results/checkpoint-37130/trainer_state.json (deflated 81%)\n  adding: kaggle/working/results/checkpoint-37130/optimizer.pt (deflated 17%)\n  adding: kaggle/working/results/checkpoint-37130/rng_state.pth (deflated 28%)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport argparse\nimport logging\nimport os\nimport json\nfrom tqdm.auto import tqdm\nfrom pathlib import Path\n\nimport random\nimport torch\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\nfrom datasets import load_metric\nfrom sklearn.metrics import f1_score\nfrom torch.utils.data import DataLoader\nfrom transformers import default_data_collator\n\ncategory_df = pd.read_csv('/kaggle/input/test-input/category.csv', dtype={'SSno': str})\nidx_to_SS = category_df.SSno.values\nSS_to_idx = {str(cat):idx for idx, cat in enumerate(idx_to_SS)}\nidx_to_SS = {value: key for key, value in SS_to_idx.items()}\n\ndef load_data():\n    category_df = pd.read_csv('/kaggle/input/test-input/category.csv', dtype={'SSno': str})\n    test_df = pd.read_csv('/kaggle/input/test-input/test_input.csv')\n    test_set = Dataset.from_pandas(test_df)\n\n    return test_set, category_df\n\ndef load_model():\n    # hyunwoongko/kobart\n    # jaehyeong/koelectra-base-v3-generalized-sentiment-analysis\n    model_path = 'hyunwoongko/kobart'\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        pretrained_model_name_or_path = model_path,\n        num_labels = 564,\n        id2label = idx_to_SS,\n        label2id = SS_to_idx,\n        ignore_mismatched_sizes=True,\n    )\n    \n    model.classification_head.dropout = nn.Identity()\n    model.load_state_dict(torch.load('/kaggle/working/results/checkpoint-55695/pytorch_model.bin'))\n    # 7428 9285 11142 12999 14856\n    return tokenizer, model\n\ndef preprocess_data(dataset):\n    def preprocess_fn(example):\n        title = example['invention_title']\n        abstract = example['abstract']\n        claims = example['claims']\n\n        #texts = f\"{title}\"#요약: {abstract} 청구항: {claims}\"\n        #texts = f\"{abstract}\"\n        texts = f\"제목: {title} 요약: {abstract} 청구항: {claims}\"\n        return {\n            'texts': texts,\n        }\n    \n    preprocessed = dataset.map(\n        preprocess_fn,\n        remove_columns=[\n            col\n            for col in dataset.column_names\n            if col not in ['documentId']\n        ],\n    )\n    return preprocessed\n\ndef tokenize_data(dataset, tokenizer):\n    def batch_tokenize(batch):\n        tokenized_batch = tokenizer(\n            batch['texts'],\n            max_length=max_length,\n            padding='max_length',\n            truncation=True,\n        )\n        tokenized_batch['documentId'] = batch['documentId']\n        return tokenized_batch\n    \n    tokenized = dataset.map(\n        batch_tokenize,\n        batched=True,\n    )\n    \n    return tokenized\n\ndef pred(dataset, model, tokenizer):\n    device = 'cuda'\n\n    test_loader = DataLoader(\n        dataset,\n        batch_size = batch_size,\n        shuffle=False,\n        collate_fn=default_data_collator,\n    )\n\n    model.to(device)\n    model.eval()\n\n    result_ids = []\n    result_logits = []\n    for batch in tqdm(test_loader):\n        with torch.no_grad():\n            outputs = model(\n                input_ids = batch['input_ids'].to(device),\n                attention_mask = batch['attention_mask'].to(device),\n            )\n            result_ids.append(batch['documentId'].numpy())\n            predictions = torch.sigmoid(outputs.logits)\n            result_logits.append(predictions.detach().cpu().numpy())\n    \n    ids = np.concatenate(result_ids)\n    logits = np.concatenate(result_logits)\n\n    return ids, logits\n\ndef get_true_indices(arr):\n    true_indices = []\n\n    for row in arr:\n        row_indices = np.where(row)[0]\n        value = []\n        for v in row_indices.tolist():\n            value.append(idx_to_SS[v])\n        true_indices.append(' '.join(value))\n    return true_indices\n\ndef filter_tensor(tensor, th):\n    max_values = np.max(tensor, axis=1)  # 텐서에서 가장 큰 값\n    filtered_tensor = tensor >= max_values[:, np.newaxis]*(1 - th)\n    \n    return filtered_tensor\n\ndef save_submission(ids, preds, category_df):\n    idx_to_SSno = category_df.SSno.values\n    \n    pred = filter_tensor(preds, 0.025)\n\n    result = get_true_indices(pred)\n    \n    submission = pd.DataFrame({'documentId':ids, 'SSnos':result})\n    submission.to_csv('submission.csv', index=False)\n    \n    cnt = 0 \n    for i in result:\n        if len(i) == 5:\n            cnt += 1\n\n    print(cnt)\n\ndef main():\n    dataset, category_df = load_data()\n    tokenizer, model = load_model()\n\n    dataset = preprocess_data(dataset)\n    dataset = tokenize_data(dataset, tokenizer)\n\n    ids, preds = pred(dataset, model, tokenizer)\n    save_submission(ids, preds, category_df)\n\nif __name__ == '__main__':\n    main()\n","metadata":{"execution":{"iopub.status.busy":"2023-07-31T01:34:00.957030Z","iopub.execute_input":"2023-07-31T01:34:00.957415Z","iopub.status.idle":"2023-07-31T01:38:07.301022Z","shell.execute_reply.started":"2023-07-31T01:34:00.957374Z","shell.execute_reply":"2023-07-31T01:38:07.299900Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\nSome weights of BartForSequenceClassification were not initialized from the model checkpoint at hyunwoongko/kobart and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"466d39155eb442f1a6e9eeb0243fe53b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14a9b4ac799c41068aacc362c816c162"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/625 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a27efd3dcaf945e6baea77741b603eb0"}},"metadata":{}},{"name":"stdout","text":"8377\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}