{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\n!pip install -q sktime\nimport sktime\nimport tqdm as tq\nimport xgboost as xgb\nimport matplotlib\nimport seaborn as sns\nimport sklearn as skl\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom sktime.forecasting.model_selection import temporal_train_test_split\nfrom sktime.utils.plotting import plot_series\nfrom xgboost import XGBRegressor\n\n# train = pd.read_csv('/kaggle/input/818818/train_818.csv')\n# valid = pd.read_csv('/kaggle/input/818818/valid_818.csv')\nnp.random.seed(42)\n\ndef SMAPE(true, pred):\n    return np.mean((np.abs(true-pred))/(np.abs(true) + np.abs(pred))) * 200\n\ndef weighted_mse(alpha = 1):\n    def weighted_mse_fixed(label, pred):\n        residual = (label - pred).astype(\"float\")\n        grad = np.where(residual>0, -2*alpha*residual, -2*residual)\n        hess = np.where(residual>0, 2*alpha, 2.0)\n        return grad, hess\n    return weighted_mse_fixed","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-05T10:22:03.061237Z","iopub.execute_input":"2023-08-05T10:22:03.061637Z","iopub.status.idle":"2023-08-05T10:22:20.903820Z","shell.execute_reply.started":"2023-08-05T10:22:03.061603Z","shell.execute_reply":"2023-08-05T10:22:20.902373Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_train2():\n    info = pd.read_csv('/kaggle/input/big-one/building_info.csv').drop(['ESS저장용량(kWh)', 'PCS용량(kW)'], axis=1)\n    info.columns = ['building', 'type', 'all_area', 'cool_area', 'sun']\n    info['sun'] = info['sun'].replace('-', 0)\n    info['sun'] = info['sun'].astype('float')\n    types = info['type'].unique()\n    value_dict = {value: index for index, value in enumerate(types)}\n    info['type'] = info['type'].map(value_dict)\n\n    info.loc[64, 'cool_area'] = 146585.0\n    info.loc[65, 'cool_area'] = 83781.0\n    info.loc[67, 'cool_area'] = 310488.0\n    info.loc[76, 'cool_area'] = 35716.0\n    info.loc[79, 'cool_area'] = 135899.6\n\n    np.random.seed(0)\n\n    cols = ['num_date_time', 'building', 'date_time', 'temp', 'prec','wind', 'hum', 'target']\n\n    train = pd.read_csv('/kaggle/input/big-one/train.csv').drop(['일조(hr)', '일사(MJ/m2)'], axis=1)\n\n    train['풍속(m/s)'] = train['풍속(m/s)'].fillna(method='ffill')\n    train['습도(%)'] = train['습도(%)'].fillna(method='ffill')\n    train = train.fillna(0)\n    train.columns = cols\n    #train = train.merge(info, on='building', how='left')\n\n    test = pd.read_csv('/kaggle/input/big-one/test.csv')\n    test.columns = cols[:-1]\n    #test = test.merge(info, on='building', how='left')\n\n    def add_data(df):\n        for i in range(2):\n            np.random.seed(i)\n            num_rows = len(df)\n\n            random_temp = df['temp'] * np.random.uniform(0.9, 1.1, num_rows)\n            random_prec = df['prec'] * np.random.uniform(0.9, 1.1, num_rows)\n            random_wind = df['wind'] * np.random.uniform(0.9, 1.1, num_rows)\n            random_hum = df['hum'] * np.random.uniform(0.9, 1.1, num_rows)\n\n            # 소수 첫째 자리까지 반올림\n            random_temp = np.round(random_temp, 1)\n            random_prec = np.round(random_prec, 1)\n            random_wind = np.round(random_wind, 1)\n            random_hum = np.round(random_hum, 1)\n\n            # 새로운 데이터프레임 생성\n\n            new_df = df.copy()\n            new_df['temp'] = random_temp\n            new_df['prec'] = random_prec\n            new_df['wind'] = random_wind\n            new_df['hum'] = random_hum\n\n            # 기존 데이터프레임과 새로운 데이터프레임을 이어붙임\n            df = pd.concat([df, new_df], ignore_index=True)\n        df = df.sort_values(by=['building', 'date_time']).reset_index(drop=True)\n        return df\n\n\n    # 시간 관련 변수들 생성\n    date = pd.to_datetime(train.date_time)\n    train['hour'] = date.dt.hour\n    train['dow'] = date.dt.weekday\n    train['month'] = date.dt.month\n    train['week'] = date.dt.isocalendar().week\n    train['day'] = date.dt.day\n\n#     avg_temp = pd.pivot_table(train[train['hour']%3 == 0], values = 'temp', index = ['building', 'day', 'month'], aggfunc = np.mean).reset_index()\n#     avg_temp.rename(columns={'temp': 'avg_temp'}, inplace=True)\n#     train = pd.merge(train, avg_temp, on=['building', 'day', 'month'], how='left')\n    \n#     max_temp = pd.pivot_table(train, values = 'temp', index = ['building', 'day', 'month'], aggfunc = np.max).reset_index()\n#     max_temp.rename(columns={'temp': 'max_temp'}, inplace=True)\n#     train = pd.merge(train, max_temp, on=['building', 'day', 'month'], how='left')\n    \n#     min_temp = pd.pivot_table(train, values = 'temp', index = ['building', 'day', 'month'], aggfunc = np.min).reset_index()\n#     min_temp.rename(columns={'temp': 'min_temp'}, inplace=True)\n#     train = pd.merge(train, min_temp, on=['building', 'day', 'month'], how='left')\n    \n    date = pd.to_datetime(test.date_time)\n    test['hour'] = date.dt.hour\n    test['dow'] = date.dt.weekday\n    test['month'] = date.dt.month\n    test['week'] = date.dt.isocalendar().week\n    test['day'] = date.dt.day\n\n#     avg_temp = pd.pivot_table(test[test['hour']%3 == 0], values = 'temp', index = ['building', 'day', 'month'], aggfunc = np.mean).reset_index()\n#     avg_temp.rename(columns={'temp': 'avg_temp'}, inplace=True)\n#     test = pd.merge(test, avg_temp, on=['building', 'day', 'month'], how='left')\n\n#     max_temp = pd.pivot_table(test, values = 'temp', index = ['building', 'day', 'month'], aggfunc = np.max).reset_index()\n#     max_temp.rename(columns={'temp': 'max_temp'}, inplace=True)\n#     test = pd.merge(test, max_temp, on=['building', 'day', 'month'], how='left')\n    \n#     min_temp = pd.pivot_table(test, values = 'temp', index = ['building', 'day', 'month'], aggfunc = np.min).reset_index()\n#     min_temp.rename(columns={'temp': 'min_temp'}, inplace=True)\n#     test = pd.merge(test, min_temp, on=['building', 'day', 'month'], how='left')\n    \n    power_mean = pd.pivot_table(train, values = 'target', index = ['building', 'hour', 'dow'], aggfunc = np.mean).reset_index()\n    power_mean.rename(columns={'target': 'dow_hour_mean'}, inplace=True)\n    train = pd.merge(train, power_mean, on=['building', 'hour', 'dow'], how='left')\n    test = pd.merge(test, power_mean, on=['building', 'hour', 'dow'], how='left')\n\n#     power_std = pd.pivot_table(train, values = 'target', index = ['building', 'hour', 'dow'], aggfunc = np.std).reset_index()\n#     power_std.rename(columns={'target': 'dow_hour_std'}, inplace=True)\n#     train = pd.merge(train, power_std, on=['building', 'hour', 'dow'], how='left')\n#     test = pd.merge(test, power_std, on=['building', 'hour', 'dow'], how='left')\n\n    # type_mean = pd.pivot_table(train, values = 'target', index = ['type', 'hour', 'dow'], aggfunc = np.mean).reset_index()\n    # type_mean.rename(columns={'target': 'type_hour_mean'}, inplace=True)\n    # train = pd.merge(train, type_mean, on=['type', 'hour', 'dow'], how='left')\n    # test = pd.merge(test, type_mean, on=['type', 'hour', 'dow'], how='left')\n\n    # type_std = pd.pivot_table(train, values = 'target', index = ['type', 'hour', 'dow'], aggfunc = np.std).reset_index()\n    # type_std.rename(columns={'target': 'type_hour_std'}, inplace=True)\n    # train = pd.merge(train, type_std, on=['type', 'hour', 'dow'], how='left')\n    # test = pd.merge(test, type_std, on=['type', 'hour', 'dow'], how='left')\n\n    ### 공휴일 변수 추가\n    test['date'] = pd.to_datetime(test['date_time'], format='%Y-%m-%d %H')\n    train['date'] = pd.to_datetime(train['date_time'], format='%Y-%m-%d %H')\n\n    train['holiday'] = train.apply(lambda x : 0 if x['dow'] < 5 else 1, axis = 1)\n    test['holiday'] = test.apply(lambda x : 0 if x['dow'] < 5 else 1, axis = 1)\n\n    train.loc[train['building'] == 3, 'holiday'] = 0\n    train.loc[(train['building'] == 3) & (train['dow'] == 0) , 'holiday'] = 1\n    train.loc[(train['building'] == 3) & (train['date_time'].str.match(r'^20220731 \\d{2}$')) , 'holiday'] = 1\n    train.loc[(train['building'] == 3) & (train['date_time'].str.match(r'^20220723 \\d{2}$')) , 'holiday'] = 1\n    train.loc[(train['building'] == 3) & (train['date_time'].str.match(r'^20220720 \\d{2}$')) , 'holiday'] = 1\n    test.loc[test['building'] == 3, 'holiday'] = 0\n    test.loc[(test['building'] == 3) & (test['dow'] == 0) , 'holiday'] = 1\n\n    train.loc[train['building'] == 2, 'holiday'] = 0\n    train.loc[(train['building'] == 2) & (train['dow'] == 0) , 'holiday'] = 1\n    train.loc[(train['building'] == 2) & (train['date_time'].str.match(r'^20220607 \\d{2}$')) , 'holiday'] = 1\n    train.loc[(train['building'] == 2) & (train['date_time'].str.match(r'^20220617 \\d{2}$')) , 'holiday'] = 1\n    test.loc[test['building'] == 2, 'holiday'] = 0\n    test.loc[(test['building'] == 2) & (test['dow'] == 0) , 'holiday'] = 1\n\n    train.loc[train['building'] == 54, 'holiday'] = 0\n    train.loc[(train['building'] == 54) & (train['dow'] == 0) , 'holiday'] = 1\n    train.loc[(train['building'] == 54) & (train['date_time'].str.match(r'^20220816 \\d{2}$')) , 'holiday'] = 1\n    train.loc[(train['building'] == 54) & (train['date_time'].str.match(r'^20220817 \\d{2}$')) , 'holiday'] = 1\n    test.loc[test['building'] == 54, 'holiday'] = 0\n    test.loc[(test['building'] == 54) & (test['dow'] == 0) , 'holiday'] = 1\n\n    train.loc[(train['date_time'].str.match(r'^20220601 \\d{2}$')) & (train['building'] != 14) , 'holiday'] = 1\n    train.loc[(train['date_time'].str.match(r'^20220606 \\d{2}$')) & (train['building'] != 14), 'holiday'] = 1\n    train.loc[(train['date_time'].str.match(r'^20220815 \\d{2}$')) & (train['building'] != 14), 'holiday'] = 1\n    train.loc[(train['building'] == 14) & (train['date_time'].str.match(r'^20220614 \\d{2}$')) , 'holiday'] = 1\n\n    def week_of_month(date):\n        first_day = date.replace(day=1)\n        if (date.week - first_day.week + 1) % 2 == 0:\n            if date.weekday() == 6:\n                return 1\n        return 0\n\n    train['week_of_month'] = train['date'].apply(week_of_month)\n    test['week_of_month'] = test['date'].apply(week_of_month)\n\n    target_buildings = [87,88,89,90,91,92]\n    train.loc[(train['building'].isin(target_buildings)) , 'holiday'] = 0\n    train.loc[(train['building'].isin(target_buildings)) & (train['week_of_month'] == 1), 'holiday'] = 1\n    test.loc[(test['building'].isin(target_buildings)) , 'holiday'] = 0\n    test.loc[(test['building'].isin(target_buildings)) & (test['week_of_month'] == 1), 'holiday'] = 1\n\n    train.loc[train['building'] == 85, 'holiday'] = 0\n    test.loc[test['building'] == 85, 'holiday'] = 0\n\n    test['date'] = pd.to_datetime(test['date_time'], format='%Y-%m-%d %H').dt.date\n    train['date'] = pd.to_datetime(train['date_time'], format='%Y-%m-%d %H').dt.date\n\n    target_day = ['2022-06-10', '2022-08-10', '2022-07-10', '2022-07-24', '2022-06-26', '2022-07-30']\n    train.loc[train['building'] == 86, 'holiday'] = 0\n    test.loc[test['building'] == 86, 'holiday'] = 0\n    for i in target_day:\n        k = pd.to_datetime(i)\n        train.loc[(train['date'] == k.date()) & (train['building'] == 86), 'holiday'] = 1\n        test.loc[(test['date'] == k.date()) & (train['building'] == 86), 'holiday'] = 1\n\n    power_holiday_mean = pd.pivot_table(train, values = 'target', index = ['building', 'hour', 'holiday'], aggfunc = np.mean).reset_index()\n    power_holiday_mean.rename(columns={'target': 'holiday_mean'}, inplace=True)\n    train = pd.merge(train, power_holiday_mean, on=['building', 'hour', 'holiday'], how='left')\n    test = pd.merge(test, power_holiday_mean, on=['building', 'hour', 'holiday'], how='left')\n\n    power_holiday_std = pd.pivot_table(train, values = 'target', index = ['building', 'hour', 'holiday'], aggfunc = np.std).reset_index()\n    power_holiday_std.rename(columns={'target': 'holiday_std'}, inplace=True)\n    train = pd.merge(train, power_holiday_std, on=['building', 'hour', 'holiday'], how='left')\n    test = pd.merge(test, power_holiday_std, on=['building', 'hour', 'holiday'], how='left')\n\n    power_hour_mean = pd.pivot_table(train, values = 'target', index = ['building', 'hour',], aggfunc = np.mean).reset_index()\n    power_hour_mean.rename(columns={'target': 'hour_mean'}, inplace=True)\n    train = pd.merge(train, power_hour_mean, on=['building', 'hour', ], how='left')\n    test = pd.merge(test, power_hour_mean, on=['building', 'hour', ], how='left')\n\n    power_hour_std = pd.pivot_table(train, values = 'target', index = ['building', 'hour',], aggfunc = np.std).reset_index()\n    power_hour_std.rename(columns={'target': 'hour_std'}, inplace=True)\n    train = pd.merge(train, power_hour_std, on=['building', 'hour', ], how='left')\n    test = pd.merge(test, power_hour_std, on=['building', 'hour', ], how='left')\n\n    # train = add_data(train)\n    # valid = add_data(valid)\n\n    ## https://dacon.io/competitions/official/235680/codeshare/2366?page=1&dtype=recent\n    train['sin_time'] = np.sin(2*np.pi*train.hour/24)\n    train['cos_time'] = np.cos(2*np.pi*train.hour/24)\n    test['sin_time'] = np.sin(2*np.pi*test.hour/24)\n    test['cos_time'] = np.cos(2*np.pi*test.hour/24)\n\n    train['THI'] = 9/5*train['temp'] - 0.55*(1-train['hum']/100)*(9/5*train['hum']-26)+32\n    #train['THI']=pd.cut(train.THI, bins=[-100, 68, 75, 80, 200], labels=['0', '1', '2', '3'])\n    test['THI'] = 9/5*test['temp'] - 0.55*(1-test['hum']/100)*(9/5*test['hum']-26)+32\n    #test['THI']=pd.cut(test.THI, bins=[-100, 68, 75, 80, 200], labels=['0', '1', '2', '3'])\n\n    train['WC']=13.12+0.6215*train['temp']-13.947*train['wind']**0.16+0.486*train['temp']*train['wind']**0.16\n    #train['WC']=pd.cut(train.WC, bins=[-100, 21, 25, 28, 31, 100], labels=[0,1,2,3,4])\n    test['WC']=13.12+0.6215*test['temp']-13.947*test['wind']**0.16+0.486*test['temp']*test['wind']**0.16\n    #test['WC']=pd.cut(test.WC, bins=[-100, 21, 25, 28, 31, 100], labels=[0,1,2,3,4])\n\n    def CDH(xs):\n        ys = []\n        for i in range(len(xs)):\n            if i < 11:\n                ys.append(np.sum(xs[:(i+1)]-26))\n            else:\n                ys.append(np.sum(xs[(i-11):(i+1)]-26))\n        return np.array(ys)\n\n    cdhs = np.array([])\n    for num in range(1,101):\n        temp = train[train['building'] == num]\n        cdh = CDH(temp['temp'].values)\n        cdhs = np.concatenate([cdhs, cdh])\n    train['CDH'] = cdhs\n\n    cdhs = np.array([])\n    for num in range(1,101):\n        temp = test[test['building'] == num]\n        cdh = CDH(temp['temp'].values)\n        cdhs = np.concatenate([cdhs, cdh])\n    test['CDH'] = cdhs\n\n#     train['THI'] = train['THI'].astype('int')\n#     train['WC'] = train['WC'].astype('int')\n#     test['THI'] = test['THI'].astype('int')\n#     test['WC'] = test['WC'].astype('int')\n    train['week'] = train['week'].astype(np.int32)\n    test['week'] = test['week'].astype(np.int32)\n\n    def new_type(i):\n        if i in [1,2,3,8,9,11,12,13,14,15]: return 0\n        elif i in [17,18,19,20,21,22,23]: return 1\n        elif i in [24,25,26,27,28,29,30,31]: return 2\n        elif i in [32,33,34,35,36]: return 3\n        elif i in [37,38,39,40,41,42,43,44]: return 4\n        elif i in [45,46,47,48,49,50,51,52]: return 5\n        elif i in [55,56,57,58]: return 6\n        elif i in [53,54,59,60]: return 7\n        elif i in [61,62,63,64,65,66,67,68]: return 8\n        elif i in [69,70,71,72,73,74,75,76]: return 9\n        elif i in [77,78,79,80,81,82,83,84]: return 10\n        elif i in [87,88,89,90,91,92]: return 11\n        elif i in [93,95,97,98,99,100]: return 12\n        else: return 13\n\n#     train['new_type'] = train['building'].apply(new_type)\n#     test['new_type'] = test['building'].apply(new_type)\n\n    # 37 : 620 711 88 617\n    # 38 : 613 725 81\n    # 39 : 718 88\n    # 40 : 718 620 617 88\n    # 41 : 627 725 88\n    # 42 : 613 822 711\n    \n    target_buildings = [37,38,39,40,41,42]\n    train.loc[(train['building'].isin(target_buildings)) , 'holiday'] = 0\n    test.loc[(test['building'].isin(target_buildings)) , 'holiday'] = 0\n    train.loc[(train['building'] == 37) & (train['date'].isin([pd.to_datetime(i).date() for i in ['2022-06-20', '2022-07-11', '2022-08-08', '2022-06-17']])), 'holiday'] = 1\n    train.loc[(train['building'] == 38) & (train['date'].isin([pd.to_datetime(i).date() for i in ['2022-06-13', '2022-07-25', '2022-08-01']])), 'holiday'] = 1\n    train.loc[(train['building'] == 39) & (train['date'].isin([pd.to_datetime(i).date() for i in ['2022-07-18', '2022-08-08']])), 'holiday'] = 1\n    train.loc[(train['building'] == 40) & (train['date'].isin([pd.to_datetime(i).date() for i in ['2022-06-20', '2022-07-18', '2022-06-17', '2022-08-08']])), 'holiday'] = 1\n    train.loc[(train['building'] == 41) & (train['date'].isin([pd.to_datetime(i).date() for i in ['2022-06-27', '2022-07-25', '2022-08-08']])), 'holiday'] = 1\n    train.loc[(train['building'] == 42) & (train['date'].isin([pd.to_datetime(i).date() for i in ['2022-06-13', '2022-07-11', '2022-08-22']])), 'holiday'] = 1\n    \n#     train.drop(train[(train['building'] == 37) & (train['date'].isin([pd.to_datetime(i).date() for i in ['2022-06-20', '2022-07-11', '2022-08-08', '2022-06-17']]))].index, inplace=True)\n#     train.drop(train[(train['building'] == 38) & (train['date'].isin([pd.to_datetime(i).date() for i in ['2022-06-13', '2022-07-25', '2022-08-01',]]))].index, inplace=True)\n#     train.drop(train[(train['building'] == 39) & (train['date'].isin([pd.to_datetime(i).date() for i in ['2022-07-18', '2022-08-08',]]))].index, inplace=True)\n#     train.drop(train[(train['building'] == 40) & (train['date'].isin([pd.to_datetime(i).date() for i in ['2022-06-20', '2022-07-18', '2022-06-17', '2022-08-08']]))].index, inplace=True)\n#     train.drop(train[(train['building'] == 41) & (train['date'].isin([pd.to_datetime(i).date() for i in ['2022-06-27', '2022-07-25', '2022-08-08']]))].index, inplace=True)\n#     train.drop(train[(train['building'] == 42) & (train['date'].isin([pd.to_datetime(i).date() for i in ['2022-06-13', '2022-07-11', '2022-08-22',]]))].index, inplace=True)\n    \n    def weather(train):\n        condition = train['prec'] > 0\n        filtered_df = train[condition].index.tolist()\n        train['weather'] = 0\n        # 3개의 1로 이루어진 열 생성\n        for idx in filtered_df:\n            if idx - 3 >= 0:\n                train.loc[idx - 3, 'weather'] = 1\n            if idx - 2 >= 0:\n                train.loc[idx - 2, 'weather'] = 1\n            if idx - 1 >= 0:\n                train.loc[idx - 1, 'weather'] = 1\n            train.loc[idx, 'weather'] = 1\n            if idx + 1 < len(train):\n                train.loc[idx + 1, 'weather'] = 1\n            if idx + 2 < len(train):\n                train.loc[idx + 2, 'weather'] = 1\n            if idx + 3 < len(train):\n                train.loc[idx + 3, 'weather'] = 1\n        return train\n    \n#     train = weather(train)\n#     test = weather(test)\n            \n            \n    train.drop(['hour', 'week_of_month', 'prec', 'day', 'date'], axis = 1, inplace = True)\n    test.drop(['hour', 'week_of_month', 'prec', 'day', 'date'], axis = 1, inplace = True)\n# type day\n    print('done')\n    return train, test\ntrain, test = get_train2()","metadata":{"execution":{"iopub.status.busy":"2023-08-05T10:22:20.906428Z","iopub.execute_input":"2023-08-05T10:22:20.906796Z","iopub.status.idle":"2023-08-05T10:22:30.801980Z","shell.execute_reply.started":"2023-08-05T10:22:20.906764Z","shell.execute_reply":"2023-08-05T10:22:30.800294Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"done\n","output_type":"stream"}]},{"cell_type":"code","source":"import optuna\nimport optuna.logging\nfrom tqdm import tqdm\noptuna.logging.set_verbosity(optuna.logging.WARNING)\n\ndf = pd.DataFrame()\n\nfor i in tqdm(range(100)):\n    y = train.loc[train.building == i+1, 'target']\n    x = train.loc[train.building == i+1, ].iloc[:, 3:].drop(['target'], axis=1)\n    y_train, y_valid, x_train, x_valid = temporal_train_test_split(y = y, X = x, test_size = 168)\n    \n    def objective(trial):\n        param = {\n            'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0),\n            'gamma': trial.suggest_float('gamma', 1e-3, 10),\n            'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0),\n            'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n            'subsample': trial.suggest_categorical('subsample', [0.6, 0.7, 0.8, 1.0]),\n            'max_depth': trial.suggest_categorical('max_depth', [3, 4, 5, 6]),\n            'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n        }\n\n        xgb = XGBRegressor(**param, n_estimators=100, learning_rate=0.01)\n        \n        xgb.fit(x_train, y_train, verbose=False)\n\n        preds = xgb.predict(x_valid)\n        smape = SMAPE(y_valid, preds)\n\n        return smape\n\n    study = optuna.create_study(direction='minimize', study_name=None)\n    study.optimize(objective, n_trials=500)\n\n    df = pd.concat([df, study.trials_dataframe().sort_values(by=['value'], ascending=[True]).head(1)]).reset_index(drop=True)\ndf.to_csv('parameters.csv', index=False)\ndf.head(5)\n# params_colsample_bytree 0.9 params_gamma 8.161415 params_max_depth 6 params_min_child_weight 47 \n# params_reg_alpha 6.721675 params_reg_lambda 6.064121 params_subsample 1","metadata":{"execution":{"iopub.status.busy":"2023-08-05T10:22:30.804640Z","iopub.execute_input":"2023-08-05T10:22:30.804999Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":" 30%|███       | 30/100 [53:36<1:58:15, 101.36s/it]","output_type":"stream"}]},{"cell_type":"code","source":"scores = []   # smape 값을 저장할 list\nbest_it = []  # best interation을 저장할 list\nfor i in tqdm(range(100)):\n    y = train.loc[train.building == i+1, 'target']\n    x = train.loc[train.building == i+1, ].iloc[:, 3:].drop(['target'], axis=1)\n    y_train, y_valid, x_train, x_valid = temporal_train_test_split(y = y, X = x, test_size = 168)\n    \n    xgb_reg = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.8, eta=0.01, gamma=0,\n             gpu_id=-1, importance_type='gain', interaction_constraints='',\n             learning_rate=0.00999999978, max_delta_step=0, max_depth=5,\n             min_child_weight=6, monotone_constraints='()',\n             n_estimators=10000, n_jobs=0, num_parallel_tree=1, random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=0,\n             subsample=0.9, tree_method='exact', validate_parameters=1,\n             verbosity=None, early_stopping_rounds=300)\n    \n#     xgb_reg = XGBRegressor(colsample_bytree=df.params_colsample_bytree[i], gamma=df.params_gamma[i], max_depth=df.params_max_depth[i],\n#                           min_child_weight=df.params_min_child_weight[i], reg_alpha=df.params_reg_alpha[i],\n#                           reg_lambda=df.params_reg_lambda[i], subsample=df.params_subsample[i], \n#                           n_estimators=10000, early_stopping_rounds=300, eval_metric=SMAPE)\n    \n    xgb_reg.set_params(**{'objective':weighted_mse(100)})\n    \n    xgb_reg.fit(x_train, y_train, eval_set=[(x_train, y_train), \n                                            (x_valid, y_valid)], verbose=False)\n    \n    y_pred = xgb_reg.predict(x_valid)  \n    \n    sm = SMAPE(y_valid, y_pred)\n    scores.append(sm)\n    best_it.append(xgb_reg.best_iteration+1) ## 실제 best iteration은 이 값에 +1 해주어야 함.\nprint(sum(scores)/len(scores)) # 10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = np.array([]) \n\nfor i in tqdm(range(100)):\n    pred_df = pd.DataFrame()   # 시드별 예측값을 담을 data frame\n    \n    for seed in [0,1,2,3,4]: # 각 시드별 예측\n        y_train = train.loc[train.building == i+1, 'target']\n        x_train = train.loc[train.building == i+1, ].iloc[:, 3:].drop(['target'], axis=1)\n        x_test = test.loc[test.building == i+1, ].iloc[:,3:]\n        \n        xgb = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n                 colsample_bynode=1, colsample_bytree=0.8, eta=0.01, gamma=0,\n                 gpu_id=-1, importance_type='gain', interaction_constraints='',\n                 learning_rate=0.00999999978, max_delta_step=0, max_depth=5,\n                 min_child_weight=6, monotone_constraints='()',\n                 n_estimators=best_it[i], n_jobs=0, num_parallel_tree=1,\n                 random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n                 seed=seed, subsample=0.9, tree_method='exact', validate_parameters=1,\n                 verbosity=None)\n\n#         xgb = XGBRegressor(colsample_bytree=df.params_colsample_bytree[i], gamma=df.params_gamma[i], max_depth=df.params_max_depth[i],\n#                       min_child_weight=df.params_min_child_weight[i], reg_alpha=df.params_reg_alpha[i],\n#                       reg_lambda=df.params_reg_lambda[i], subsample=df.params_subsample[i], \n#                       n_estimators=best_it[i])\n#         if xgb_params.loc[index, 'alpha'] != 0:\n#             xgb.set_params(**{'objective':weighted_mse(xgb_params.loc[index, 'alpha'])})\n        \n        xgb.fit(x_train, y_train)\n        y_pred = xgb.predict(x_test)\n        pred_df.loc[:,seed] = y_pred   # 각 시드별 예측 담기\n        \n    pred = pred_df.mean(axis=1)        # (i+1)번째 건물의 예측 =  (i+1)번째 건물의 각 시드별 예측 평균값\n    preds = np.append(preds, pred)   \n    \nsubmission = pd.read_csv('/kaggle/input/big-one/sample_submission.csv')\nsubmission['answer'] = preds\nsubmission.to_csv('./submission_xgb.csv', index = False)\nsubmission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}